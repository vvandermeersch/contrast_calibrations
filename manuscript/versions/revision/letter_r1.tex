\DocumentMetadata{testphase=new-or-1}
\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{bibentry}
\usepackage[round,sort,comma]{natbib}
\bibliographystyle{plainnat}
\usepackage{gensymb}

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\large\textbf{Response to Reviewers} \hfill \textbf{ECOMOD-24-141} \\
\normalsize Ecological Modelling \hfill \textit{} \\

\noindent\hrulefill
\tableofcontents
\noindent\hrulefill

\renewcommand*\thetable{R\arabic{table}}
\renewcommand*\thefigure{R\arabic{figure}}

\definecolor{customblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{customred}{rgb}{0.759, 0.365, 0.396}

\newcommand{\textappr}{\raisebox{0.5ex}{\texttildelow}}

\section{General Comment from the Editor}

Dear authors,

\noindent Thank you for submitting your manuscript to our journal. We have now received two constructive and detailed reviews. I fully agree with both. \\
\textcolor{customblue}{Thank you for giving us the opportunity to revise our manuscript. We have worked to clarify the points raised by the 2 reviewers in this new version. In the following, our responses to the reviewer comments will appear \textbf{in blue} and the changes we made in the text \textbf{\textcolor{customred}{in red}}.}\\


\noindent In addition to the reviews, I have a request: it is wonderful that you have provided all the data, model codes as R scripts, etc. in your GitHub folder. However, since you did not mention the use of the R environment and the availability of the code in the 'Materials and methods' section of your manuscript, it took a while for the reader to find out that you had provided all the necessary information on Github. Although the 'Data availability' section contains all the necessary information, it would be helpful if you could include a note at the beginning of the 'Methods' section.\\
\textcolor{customblue}{Thank you for this suggestion. We now mentioned the GitHub repsitory at the beginning of this section.}\\

\clearpage

\section{Response to Reviewer 1}
In this manuscript, Van der Meersch and Chuine explored the potential of using an inverse calibration method to improve a species distribution model of forest trees that is based on phenological processes. Considering that process-based models are key to predict the future composition and functioning of forests, it is essential to calibrate them accurately. In complement to direct calibration approaches, inverse calibration approaches have flourished over the last years thanks to the growing size, quality and accessibility of ecological database, and to the huge progresses in computer sciences. If the latter method has advantages, it also has drawbacks; both being well described and highlighted in the manuscript. The paper is globally well written ; however I have some comments and suggestions that may/should be considered before publication of the manuscript.\\
\textcolor{customblue}{Thank you sincerely for your thoughtful comments. We are pleased to hear that you found the aim of the work relevant and the manuscript well-written. We have carefully considered your concerns and have made every effort to address them comprehensively.}\\

\subsection{Methodological questionings}

P6-L179-184: Just to be clear: each of these 100 calibrations have been run with a different occurrence subset? And, in this subset, only the presences have been sampled, and the model consider all absences without randomly sampling them? I'm asking that to better understand if the variability in model outputs depends on the variability in the sampling characteristics or on the calibration process itself. \\
\textcolor{customblue}{Thank you for this question. We realize it was indeed not clear in the text. The 100 calibrations were run on 10 different subsets. There were thus 10 repetitions of the calibration by subset. To create the 10 subsets, we randomly sampled the same number of absences than presences. We have made this point clearer in the manuscript.}\\
In the text,\\ 
- lines 192-193: \textcolor{customred}{"We then randomly sampled the same number of absences."}\\ 
- lines 197-198: \textcolor{customred}{"on 10 different presence/absence subsets (i.e. 10 calibrations per subset)"}\\

\noindent The answer to this question will determine the interpretation of the Fig. 5: Considering the strong variation in model outputs at intra-specific level (100 simulations on beech; Fig. 3), I'm wondering if the differences among species shown in Fig. 5 are just obtained by chance (5 simulations * 2 occurrence dataset only) or not.\\
\textcolor{customblue}{This is an interesting question. However, note that the strong variations in Fig. 3 are obtained with a \emph{full} inverse calibration---all parameters are fitted at once---whereas in Fig. 5 only few parameters are calibrated. The main difference among species in the Fig. 5 is that $FH^{min}$ parameter for \emph{Betula pendula} appears to vary more than for the other two species, and the RMSE of the flowering date for \emph{Betula pendula}  is also more variable. While we indeed only run 10 calibrations (because of computational constraints), we believe that this could pinpoint some issues with current model for  \emph{Betula pendula} (rather than being obtained by chance). This is the species for which partial calibration is the least effective (mean AUC = 0.78), possibly due to its particularly wide range.}\\

\clearpage

\noindent In the Discussion or in the M\&M, I would appreciate some sentences on the advantages/limits of your CMA-ES method in comparison to other inverse calibration approaches, such as the Bayesian one (as done in your precedent paper in Methods in Ecol. and Evol., but in a shorter version).\\
\textcolor{customblue}{Thank you for this suggestion.}\\
In the text, lines 172-177: \textcolor{customred}{"CMA-ES is easy to use, as it requires minimal tuning to effectively explore the parameter space. Global optimization algorithms such as CMA-ES are commonly used to calibrate complex models, but are not as robust as proper statistical inference (e.g., Bayesian methods) and primarily focus on identifying the most fit parameter set."}\\ 

\noindent I understand the choice of using RMSE to assess the accuracy of the predictions in comparison with the observations (e.g., in Figs. 3 and 5). However, it doesn't highlight if the model systematically overestimates or underestimated any date (such as the leafout date). Would that be possible to include such information ?\\
\textcolor{customblue}{Yes, we totally agree with your point.  We now mention this aspect in the Results.}\\
In the text, 
- lines 281-282: \textcolor{customred}{"Both tend to overestimate the leafout day (with an average bias of 24.0$\pm$25.9 days)"}\\ 
- lines 294-295: \textcolor{customred}{"On average, inverse calibrated models tend to overestimate the senescence date, with a bias of 16.9$\pm$25.9 days."}\\ 

\subsection{Other comments}

P1-L20 : 'more'… in comparison to what?\\
\textcolor{customblue}{We remove the word 'more' which was a clumsiness.}\\

\noindent P2-L16: can you add a reference supporting your statement that carbon allocation is not well represented in ecological models?\\
\textcolor{customblue}{Our knowledge of the regulatory mechanisms that control fluxes among the different metabolic processes and pools is indeed still limited. We added a reference to support our statement.}\\
In the text, line 16: \textcolor{customred}{"\citep{Hartmann2020}"}\\ 

\noindent P2-L20: reference at the end of the sentence?\\
\textcolor{customblue}{Corrected.}\\

\noindent P2-L23: "Process-explicit modeling depends a great deal on the calibration and the availability of data" this seems also true for correlative models…\\
\textcolor{customblue}{We agree. We precise our idea to better reflect that process-explicit models need many different data (not only species distribution data as correlative models do).}\\
In the text, line 24: \textcolor{customred}{"many different data"}\\ 

\noindent P4-L88 : there are numerous definitions of fitness, but I'm not sure that this one is correct.\\
\textcolor{customblue}{We slightly modified the sentence.}\\
In the text, line 93: \textcolor{customred}{"with a proxy of the fitness"}\\ 

\noindent P6-L158: inverse or direct modelling?\\
\textcolor{customblue}{Phenological submodels are inverse-calibrated using phenological data, related to each submodel}\\
In the text, line 163: \textcolor{customred}{"inverse calibration"}\\ 

\noindent Figure 1: would that be possible to show the actual presence of beech, and not only simulated information? \\
\textcolor{customblue}{Thank you for this idea. We added the map of current distribution, based on \citet{Caudullo2017}.}\\

\noindent Also, your overall approach aims at maximizing the AUC, while a presence/absence threshold has been determined here to derive a TSS and then build the maps. So, I'm wondering why the authors didn't choose to directly maximize TSS by optimizing the model parameters and this threshold all together.\\
\textcolor{customblue}{Because of the computational challenge of running many process-based model simulations, we had to calibrate the model on a subset of 2000 presences/absences (see Methods), whereas the threshold is typically computed using the entire dataset (of presences/absences). We thus chose to focus on AUC because it is threshold-independent measure, and we thus maximize the ability of the model to discriminate between presences and absences regardless of the threshold. However, as highlighted in our previous paper in MEE, we believe that it would be interesting to investigate the effects of choosing a different objective function. In particular, if one has a strong expert opinion on the threshold discriminating presences and absences, it would be possible to fix its value a priori.}\\

\noindent P15-L358-364: there are also legacies of forest management for silver fir, whose climatic niche seems actually larger than the realized one (e.g., see Tinner et al. 2013), and for spruce which has been probably planted outside of its niche (areas where the species is currently declining).\\
\textcolor{customblue}{We agree and have now mentioned this aspect for these two species.}\\
In the text, lines 388-390: \textcolor{customred}{"Similar legacies of forest management exist for silver fir (\emph{Abies alba}), whose climatic niche appears broader than its realized distribution \citep{Tinner2013}, and for spruce (\emph{Picea abies}),  which is a commercially very important species."}\\ 

\noindent P16-L371-374: if the input data is correct as well…\\
\textcolor{customblue}{We agree. We added this point!}\\
In the text, line 400: \textcolor{customred}{"and if the data used is correct"}\\ 

\noindent P16-L382: add 'in the PHENOFIT model' after 'fitness'.\\
\textcolor{customblue}{Done.}\\

\noindent P16-L387: and a carbon allocation scheme?\\
\textcolor{customblue}{We are not sure to understand how a more precise carbon allocation scheme would modify the leaf senescence date. However, leaf senescence could have an impact on the net primary production if it was included in the model, but the impact of a later senescence date on primary production would likely be very small.}\\

\noindent P17-L427-429: I'm pretty sure that some PEMs do so (e.g., Rey et al. 2016; Oberpiller et al. 2021)\\
\textcolor{customblue}{Sorry, we are not very familiar with the 4C model or LPJ-GUESS model. In the 4C model, the only mention of "leaf age" in \href{https://www.pik-potsdam.de/4c/web_4c/documents/4C_\%20description.pdf}{the 182-page model description}  relates to the specific leaf area. The \emph{late frost danger index} seems to depend only on the bud burst date (for deciduous tree species). For LPJ-GUESS, the most detailed representation of frost damage we found seems to be in LPJ-GUESS-FROST \citep{Meyer2024}, where late frost damage depends also only on whether bud burst has already occurred (with no mention of leaf maturity). We acknowledge that we may have overlooked something, so if you had a more specific aspect in mind, we would be happy to include it.}\\

\noindent The Figure A.6. could be inserted in the main text\\
\textcolor{customblue}{Thank you for this suggestion. We have inserted the Figure in the Methods section.}\\

\section{Response to Referee 2}

\subsection{General comments}

The title could be more representative of the content. The authors effectively tested calibration (and therefore the predictions), not the models themselves (e.g. different mechanistic structures). So, maybe the title could evoke model calibration more explicitly.\\
\textcolor{customblue}{We appreciate your suggestion and understand your concern. However, we believe that models are defined by both their mathematical structures (which were not tested here) and the values of their parameters (which we calibrated in this study). In the title, we explicitly include the word "(inverse) calibration" referring to the process of inferring parameter values. Therefore, we are not entirely sure we understand your point of view, but we are open to further clarification.}\\

\noindent The Abstract requires some re-wording to better reflect the experimental design (see specific comments below).\\
\textcolor{customblue}{We have worked to reformulate the Abstract, following your recommandations.}\\
New abstract: \textcolor{customred}{"Process-explicit models (PEMs) are expected to provide reliable projections of species range shifts because they explicitly model the biological mechanisms that drive species responses to climate.
However, their application is often limited by the need for diverse and detailed datasets, which are only available for a limited number of species. Inverse calibration has been identified as an avenue to help calibrate PEMs for many species, but it is still unclear whether it can provide biologically meaningful parameter estimates.
Here, we investigated the potential of inverse calibration techniques to improve the accuracy of PEMs. We examined the discrepancies in parameter estimates obtained by classical and inverse calibration approaches. We evaluated two inverse calibration strategies: (i) calibrating all parameters simultaneously and (ii) focusing only on critical parameters.  
We assessed the realism of the obtained parameter estimates and the simulated processes by comparing them with measurements and observations across Europe. We show that when the entire model is calibrated at once, the model structure alone may not sufficiently constrain parameter estimation, leading to unrealistic parameter values. However, selective application of the inverse calibration approach---focusing on critical parameters---can improve model performance while still simulating realistic biological mechanisms."}\\ 

\noindent The calibration experiments being compared are great and well designed. Maybe one could add more details on the expert-based calibration (that is, what parameters were measured, found in the literature). It seems that some submodels of this strategy are already inverted. Here, it is important to be clear what submodel and what data was used.\\
\textcolor{customblue}{Thank you. We have detailed on how the parameters in the classical version of the model are obtained.}\\
In the text, lines 162-167: \textcolor{customred}{"Phenology submodel parameters were inferred by inverse \textcolor{customred}{calibration} using phenological \textcolor{customred}{observations} across Europe (provided through the TEMPO data portal \url{data.pheno.fr}, and the PEP725 database \url{pep725.eu}).
\textcolor{customred}{Frost hardiness submodel parameters were directly measured or found in the literature. Water stress parameters were inferred from the specific range of annual precipitation where species are found in forest inventories.}"}\\

\noindent It is an important outcome that the inverse modelling via AI can be accurate, even with wrong estimated parameters. In this account, the next must be always explicit that this was the for the specific method used in this manuscript not of inverse modelling in general. Sentences like that of line 340, which just mention 'inverse calibration' without specifying the calibration algorithm and strategy used can be misleading. That is because other functions of likelihood comparisons and/or algorithm may be better in inverting the model, so the text should not generalize. It could also be that the data used was just not enough to identify the parameters adequately. Hence, the discussion text needs to integrate these nuances.\\
\textcolor{customblue}{Sorry. We tried to be more specific when needed.}\\
In the text:\\
- line 349: \textcolor{customred}{"our inverse calibration framework"}\\ 
- line 349: \textcolor{customred}{"the optimization algorithm CMA-ES"}\\ 
- line 395: \textcolor{customred}{"using species occurrence data"}\\ 

\noindent In regard of previous comment. Maybe the authors can also discuss potential alternative methods for automatic calibration and necessary data that follow up studies could try. Potentially if informing the algorithm with both occurrence data and phenological data could be enough to identify the parameters.\\
\textcolor{customblue}{We now mention these aspects in the text.}\\
In the text:\\
- lines 174-177: \textcolor{customred}{"Global optimization algorithms such as CMA-ES are commonly used to calibrate complex models, but are not as robust as proper statistical inference (e.g., Bayesian methods) and primarily focus on identifying the most fit parameter set."}\\
- lines 480-482: \textcolor{customred}{"It would also require inference methods that can accomodate unbalanced data streams along with model structural assumptions \citep{Oberpriller2021}"}\\

\subsection{Specific and minor points}

\noindent Abstract: P 36-37: can you clarify how the model structure plays a role in both the (i) and (ii) strategies outlined a couple of sentences earlier. Was inverting the model in both strategies an improvement? What does parsimoniously mean in this case, strategy (i) or (ii)? Please, clarify.\\
\textcolor{customblue}{We have worked to reformulate the Abstract, please see above.}\\

\noindent Please check spaces before and after commas and periods.\\
\textcolor{customblue}{Checked.}\\

\noindent Main text, l. 30: much earlier attempts of inverted mechanistic distribution models:
Cabral \& Schurr 2010, Pagel \& Schurr 2012. Maybe the authors can speculate about the reason of this big temporal gap in inverting distribution models.\\
\textcolor{customblue}{Thank you for these references. The preprint we cite line 30 is a methodological review of calibration/uncertainty analysis for ecological process-based models, and thus not an attempt to calibrate one specific model (such as RangeShifter). We included your references later in the text (see below).}\\

\clearpage
\noindent Also, Rangeshifter papers before that cited one tended to calibrate their experiments without inverting (that is calibrating directly from trait data). Can you also comment on this? Were those calibrations reliable (without inverting)? Same questionings go for the cited calibrated phenological models (l. 39). Maybe also useful for the discussion session.\\
\textcolor{customblue}{Thank you for this suggestion. Our results (figures 3 and 5) and discussion actually already highlight and investigate the differences between parameter estimates of a PEM obtained with classical calibration (for example parameter estimates of phenological models obtained with phenological observations) \textit{versus} with inverse calibration with species distribution data, but as you said below, this point was not clear in the Abstract. We hope we have made it clearer now.}\\

\noindent L. 44-54: all true and important. However, there is evidence that inverting models can deal with structural misspecifications, often generating good fit to data, but integrating slight changes in inverted parameter values that accommodate the structural/model errors or uncertainty (Oberpriller et al. 2021). 
\textcolor{customblue}{Thank you for this reference. We added it in the Discussion.}\\
In the text,lines 480-482: \textcolor{customred}{"It would also require inference methods that can accomodate unbalanced data streams along with model structural assumptions \citep{Oberpriller2021}"}\\

\noindent That begs the question: for inverted models, how much deviance from the truth (estimated parameter values vs. independent parameter estimates) are we ready to accept if the predictions still match data? $\rightarrow$ maybe rather worth revisiting this in the discussion.\\
\textcolor{customblue}{
This is indeed a very interesting point. In \citet{Oberpriller2021}, the model with the "correct" structure consistently performed best, highlighting that the process-explicit function always matters (even with robust bias correction methods). No process in a model has a "correct" structural form, it is always an approximation of what we assume is occurring at a given biological level. The choice of a functional form is generally guided by our knowledge, and we try to choose a functional form that makes sense from a biological perspective, in particular with parameters that have a biological meaning. However, if inverse calibration is not strongly constrained by model hypotheses---i.e. if it is flexible enough to accommodate any structural form---we risk losing this biological meaning. The predictions may match the data (at least within the specific context of inverse calibration), but we might lose one of the key advantages of process-explicit models over more flexible approaches, that is the mechanistic understanding.}\\
In the text, lines 378-380: \textcolor{customred}{"This is crucial to ensure that process-explicit models keep their biological meaning, which is one of their key advantages over more flexible methods that lack mechanistic understanding."}\\

\noindent L. 55-57: see suggested references. Also, is that a matter of alignment or a matter of data availability and computational power?\\
\textcolor{customblue}{From our understanding, Cabral \& Schurr (2010) used only abundance data, while Pagel \& Schurr (2011) combined both species distribution data and abundance data. We therefore only added the reference to Pagel \& Schurr (2011).\\
Species distribution data have been available for a long time, so we do not think it is a matter of data availability. However, it could have been a question of computational power until recently.}\\
In the text,\\
- line 40: \textcolor{customred}{"Pagel and Schurr, 2011"}\\
- line 57: \textcolor{customred}{"Pagel and Schurr, 2011"}\\

\clearpage
\noindent L. 71: this comparison is great, but it is not clear from the abstract. Please re-formulate the abstract to reflect this.\\
\textcolor{customblue}{Thank you for this suggestion. We have modified the Abstract so that it better reflect what is presented in the article.}\\

\noindent L. 122-123: also with the (local) environment?\\
\textcolor{customblue}{Sorry it was not clear. We have reformulated this sentence.}\\
In the text, lines 127-129: \textcolor{customred}{"Development rates ($R$) are response functions to daily temperature that can be linear or nonlinear (usually with a single optimum), with their functional form potentially varying across species and developmental phases."}\\

\noindent L. 182: why 100 calibrations? Does each calibration provide an optimized set of parameter values. Do you compare convergence or similarity among those calibrations? (this seems clearer once the reader reaches the results, but it would be nice to make this clearer at this point)\\
\textcolor{customblue}{Thank you for this suggestion. We tried to make our goal clearer in this part.}\\
In the text:\\
- lines 194-198: \textcolor{customred}{"First, we aimed to investigate to what extent fully calibrating the model using only species distribution data could improve its performance as well as the estimation of the parameter values obtained with the expert calibration. We compared one hundred parameter sets for \emph{Fagus sylvatica}, obtained by inverse calibration on 10 different presence/absence subsets (i.e. 10 calibrations per subset) [...]"}\\
- lines 202-205: \textcolor{customred}{"Second, we wanted to determine whether inverse calibration using species distribution data could improve some parts of the model where expert calibration may fall short due to data limitations. We conducted a second set of inverse calibrations, focusing only on some model parameters, for eight different species [...]"}\\

\noindent L. 194: Were these 'factors' the three 'sub-components of fitness'? Please, be consistent with the terms to avoid confusion.\\
\textcolor{customblue}{Yes. Modified.}\\

\noindent L. 214: you mean weather generator?\\
\textcolor{customblue}{Sorry for this typo. Corrected.}\\

\noindent Figure 1e: what correlative models? Correlative models have not been mentioned in the main text. Please, clarify.\\
\textcolor{customblue}{We added correlative models in Figure 1e as a point of comparison, but we understand it might be confusing. We have removed it.}\\

\noindent Figure 2: very interesting pic. What makes the calibrations differ? Is that due to initial combinations?\\
\textcolor{customblue}{Thank you. Both the subset of data used for calibration and the stochasticity of the optimization algorithm play a role in the divergence between calibrations. These strong differences highlight the non-identifiability in model parameters (i.e. the existence of several optima).}\\

\noindent Figure 4: very cool results. I wonder whether might be useful to compare the partial calibrations of the species with multiple processes calibrated, but than calibrating only one or two of the processes. This can be also informative to pick up the process profiting from inversion the most.\\
\textcolor{customblue}{Thank you very much! There is indeed still a lot to explore.}\\

\noindent Figure 5: please use italic font for species names. Check in main text throughout as well (e.g. l. 287).\\
\textcolor{customblue}{Sorry. Modified.}\\

\noindent L. 382-387: I really like how the authors used the outcomes to explain the eco-physiology and potential associated model improvements. Great paragraph conclusion. More of this.\\
\textcolor{customblue}{Thank you.}\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\renewcommand\refname{References}
\bibliography{phd_bibliography}

\end{document}